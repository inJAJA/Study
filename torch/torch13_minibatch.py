'''
Mini_Batch
: 배치 크기는 보통 2의 제곱수를 사용합니다. ex) 2, 4, 8, 16, 32, 64... 
=> CPU와 GPU의 메모리가 2의 배수 : 배치크기가 2의 제곱수일 경우에 데이터 송수신의 효율을 높일 수 있음.
'''

# iteration
# : 한 번의 에포크 내에서 이루어지는 W, b의 업데이트 횟수

import torch

x_train = torch.FloatTensor([[73, 80, 75],
                             [93, 88, 93],
                             [89, 91, 90],
                             [96, 98, 100],
                             [73, 66, 70]])
y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])


