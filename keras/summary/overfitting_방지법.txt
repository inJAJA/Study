#### 과적합 막는 법 #####
1. 훈련데이터 늘린다.
2. feature를 늘린다
3. 정규화 시킨다. 

--------------------------------------------------------------------------
#1. regularizer 
: 정규화  # 2가지 이상 사용하는 것은 권장하지 않음 = 큰 효과 없어서
: 과적합 줄임
                                           ( 규제 값 )
L1 규제 : 가중치의 절대값 합 = regularizer.l1(l = 0.01)
L2 규제 : 가중ㅊ;의 제곱 합   = regularizer.l2(l = 0.01)

       (규제 내용) 
loss =     L1     * reduce_sum(abs(x))     -> L1 * x의 절대값의 합
loss =     L2     * reduce_sum(square(x))  -> L2 * x의 제곱의 합

# L1
: 작은 W(가중치)들을 거의 0으로 수렴시켜 몇개의 중요한 가중치들만 남긴다.
: sparse model(coding)에 적합

# L2
: 전체적으로 W(가중치)값이 작아지도록 한다. / L1처럼 일부 항의 계수를 0으로 만들지 않음

from keras.regularizers import l1, l2, l1_l2
model.add(Conv2D(32, kernel_size = 3, padding = 'same', activation = 'relu', kernel_regularizer=l2(0.001)))
                                                                             
--------------------------------------------------------------------------------
#2. Dropout


#3. BatchNormalization
: 과적합 방지
: Activation 이전에 사용해야 한다.
: 원래 목적 BatchNormalization로 정규화한 값들을 활성화함수로 보내준다.
: Activation 이후에 쓰면 활성화함수를 통과한 값에 적용된다

:배치 정규화는 미니배치(mini-batch)를 단위로 데이터의 분포가 평균(​mean)이 0, 분산(​variance)이 1이 되도록 정규화(normalization)한다.

[참고] https://de-novo.org/2018/05/28/batch-normalization-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0/


from keras.layers import BatchNormalization, Activation
model.add(Conv2D(32, kernel_size = 3, padding = 'same', input_shape = (32, 32, 3)))
model.add(BatchNormalization())
model.add(Activation('relu'))                         # 따로 명시해주면 위에 default값으로 적용 x
